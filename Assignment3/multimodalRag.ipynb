{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal RAG Assignment\n",
    "\n",
    "This notebook implements a multimodal Retrieval-Augmented Generation (RAG) pipeline for processing multiple PDFs (at least 200 pages) containing text, images, and tables. The pipeline includes semantic chunking, embedding storage in Milvus with Flat, HNSW, and IVF indexes, retriever time comparison, accuracy evaluation, reranking with BM25 and MMR, and rendering output to a DOCX file.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Install the required libraries:\n",
    "```bash\n",
    "pip install unstructured[pdf] langchain langchain-openai pymilvus rank_bm25 python-docx PyPDF2 sentence-transformers\n",
    "```\n",
    "\n",
    "Set up environment variables for OpenAI API key and Milvus connection.\n",
    "\n",
    "## Prerequisites\n",
    "- A directory (`data/pdfs/`) with PDFs totaling at least 200 pages.\n",
    "- A running Milvus server (local or remote).\n",
    "- OpenAI API key for embeddings and LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from IPython.display import HTML, display\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.text_splitter import SemanticChunker\n",
    "from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from docx import Document as DocxDocument\n",
    "from docx.shared import Inches\n",
    "import PyPDF2\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['PATH'] += os.pathsep + '/opt/homebrew/bin'  # For Poppler\n",
    "\n",
    "# Directory containing PDFs\n",
    "PDF_DIR = 'data/pdfs/'\n",
    "EXTRACTED_DATA_DIR = 'extracted_data/'\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(PDF_DIR, exist_ok=True)\n",
    "os.makedirs(EXTRACTED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Milvus connection\n",
    "MILVUS_HOST = 'localhost'\n",
    "MILVUS_PORT = '19530'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Fetch Data from PDFs\n",
    "\n",
    "Iterate through all PDFs in the specified directory and extract text, images, and tables using the `unstructured` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pdf_pages(pdf_dir):\n",
    "    \"\"\"Count total pages across all PDFs in the directory.\"\"\"\n",
    "    total_pages = 0\n",
    "    for pdf_file in Path(pdf_dir).glob('*.pdf'):\n",
    "        with open(pdf_file, 'rb') as f:\n",
    "            pdf = PyPDF2.PdfReader(f)\n",
    "            total_pages += len(pdf.pages)\n",
    "    return total_pages\n",
    "\n",
    "def extract_pdf_data(pdf_dir, extracted_data_dir):\n",
    "    \"\"\"Extract text, images, and tables from all PDFs.\"\"\"\n",
    "    texts = []\n",
    "    tables = []\n",
    "    images = []\n",
    "    \n",
    "    total_pages = count_pdf_pages(pdf_dir)\n",
    "    if total_pages < 200:\n",
    "        raise ValueError(f\"Total pages ({total_pages}) is less than 200. Please add more PDFs.\")\n",
    "    \n",
    "    for pdf_file in Path(pdf_dir).glob('*.pdf'):\n",
    "        elements = partition_pdf(\n",
    "            filename=str(pdf_file),\n",
    "            strategy='hi_res',\n",
    "            extract_images_in_pdf=True,\n",
    "            extract_image_block_types=['Image', 'Table'],\n",
    "            extract_image_block_to_payload=False,\n",
    "            extract_image_block_output_dir=extracted_data_dir\n",
    "        )\n",
    "        \n",
    "        for element in elements:\n",
    "            if element.__class__.__name__ == 'Text' or element.__class__.__name__ == 'NarrativeText' or element.__class__.__name__ == 'Title':\n",
    "                texts.append(str(element))\n",
    "            elif element.__class__.__name__ == 'Table':\n",
    "                tables.append(str(element))\n",
    "            elif element.__class__.__name__ == 'Image':\n",
    "                # Read the saved image file\n",
    "                image_path = os.path.join(extracted_data_dir, f'{element.metadata.image_path.split(\"/\")[-1]}')\n",
    "                with open(image_path, 'rb') as img_file:\n",
    "                    img_data = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "                    images.append(img_data)\n",
    "    \n",
    "    return texts, tables, images\n",
    "\n",
    "# Extract data\n",
    "texts, tables, images = extract_pdf_data(PDF_DIR, EXTRACTED_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Semantic Chunking\n",
    "\n",
    "Apply semantic chunking to the extracted text to create meaningful chunks for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_chunking(texts, embedding_function):\n",
    "    \"\"\"Apply semantic chunking to texts.\"\"\"\n",
    "    text_splitter = SemanticChunker(embedding_function, breakpoint_threshold_type='percentile')\n",
    "    chunked_texts = []\n",
    "    for text in texts:\n",
    "        chunks = text_splitter.split_text(text)\n",
    "        chunked_texts.extend(chunks)\n",
    "    return chunked_texts\n",
    "\n",
    "# Initialize embedding function\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "\n",
    "# Chunk texts\n",
    "chunked_texts = semantic_chunking(texts, embedding_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Summaries for Tables and Images\n",
    "\n",
    "Generate summaries for tables and images to use as indexing content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summaries(content, content_type, model):\n",
    "    \"\"\"Generate summaries for tables or images using an LLM.\"\"\"\n",
    "    summaries = []\n",
    "    for item in content:\n",
    "        if content_type == 'table':\n",
    "            prompt = f\"Summarize the following table content in a concise manner:{item}\n",
    "\n",
    "Summary:\"\n",
    "            response = model.invoke(prompt)\n",
    "            summaries.append(response.content)\n",
    "        elif content_type == 'image':\n",
    "            prompt = [HumanMessage(content=[\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{item}\"}},\n",
    "                {\"type\": \"text\", \"text\": \"Describe this image in a concise manner.\"}\n",
    "            ])]\n",
    "            response = model.invoke(prompt)\n",
    "            summaries.append(response.content)\n",
    "    return summaries\n",
    "\n",
    "# Initialize LLM for summaries\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o\", max_tokens=1024)\n",
    "\n",
    "# Generate summaries\n",
    "table_summaries = generate_summaries(tables, 'table', llm)\n",
    "image_summaries = generate_summaries(images, 'image', llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Store in Milvus Vector Database\n",
    "\n",
    "Store embeddings in Milvus with three collections, each using a different index type (Flat, HNSW, IVF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_milvus_collection(collection_name, index_type):\n",
    "    \"\"\"Create a Milvus collection with the specified index type.\"\"\"\n",
    "    fields = [\n",
    "        FieldSchema(name=\"id\", dtype=DataType.VARCHAR, is_primary=True, max_length=36),\n",
    "        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=1536),  # OpenAI embedding dimension\n",
    "        FieldSchema(name=\"content\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "        FieldSchema(name=\"content_type\", dtype=DataType.VARCHAR, max_length=50)\n",
    "    ]\n",
    "    schema = CollectionSchema(fields=fields, description=f\"{collection_name} collection\")\n",
    "    collection = Collection(name=collection_name, schema=schema)\n",
    "    \n",
    "    if index_type == 'FLAT':\n",
    "        index_params = {\"metric_type\": \"L2\", \"index_type\": \"FLAT\", \"params\": {}}\n",
    "    elif index_type == 'HNSW':\n",
    "        index_params = {\"metric_type\": \"L2\", \"index_type\": \"HNSW\", \"params\": {\"M\": 16, \"efConstruction\": 200}}\n",
    "    elif index_type == 'IVF_FLAT':\n",
    "        index_params = {\"metric_type\": \"L2\", \"index_type\": \"IVF_FLAT\", \"params\": {\"nlist\": 1024}}\n",
    "    \n",
    "    collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "    collection.load()\n",
    "    return collection\n",
    "\n",
    "def store_in_milvus(texts, tables, images, text_summaries, table_summaries, image_summaries, index_type):\n",
    "    \"\"\"Store embeddings in a Milvus collection.\"\"\"\n",
    "    collection_name = f\"mmrag_{index_type.lower()}\"\n",
    "    if utility.has_collection(collection_name):\n",
    "        utility.drop_collection(collection_name)\n",
    "    \n",
    "    collection = create_milvus_collection(collection_name, index_type)\n",
    "    \n",
    "    # Combine all content\n",
    "    all_contents = texts + table_summaries + image_summaries\n",
    "    all_originals = texts + tables + images\n",
    "    content_types = ['text'] * len(texts) + ['table'] * len(tables) + ['image'] * len(images)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = embedding_function.embed_documents(all_contents)\n",
    "    \n",
    "    # Insert data\n",
    "    entities = [\n",
    "        [str(uuid.uuid4()) for _ in range(len(all_contents))],\n",
    "        embeddings,\n",
    "        all_originals,\n",
    "        content_types\n",
    "    ]\n",
    "    collection.insert(entities)\n",
    "    \n",
    "    return collection\n",
    "\n",
    "# Connect to Milvus\n",
    "connections.connect(host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "\n",
    "# Store data in three collections\n",
    "flat_collection = store_in_milvus(chunked_texts, tables, images, chunked_texts, table_summaries, image_summaries, 'FLAT')\n",
    "hnsw_collection = store_in_milvus(chunked_texts, tables, images, chunked_texts, table_summaries, image_summaries, 'HNSW')\n",
    "ivf_collection = store_in_milvus(chunked_texts, tables, images, chunked_texts, table_summaries, image_summaries, 'IVF_FLAT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Retriever Pipeline\n",
    "\n",
    "Create a custom retriever for Milvus that retrieves text, tables, and images based on query embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MilvusRetriever:\n",
    "    def __init__(self, collection, embedding_function):\n",
    "        self.collection = collection\n",
    "        self.embedding_function = embedding_function\n",
    "    \n",
    "    def invoke(self, query, top_k=5):\n",
    "        query_embedding = self.embedding_function.embed_query(query)\n",
    "        search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}} if 'IVF' in self.collection.name else {\"metric_type\": \"L2\"}\n",
    "        results = self.collection.search(\n",
    "            data=[query_embedding],\n",
    "            anns_field=\"embedding\",\n",
    "            param=search_params,\n",
    "            limit=top_k,\n",
    "            output_fields=[\"content\", \"content_type\"]\n",
    "        )\n",
    "        docs = []\n",
    "        for hits in results:\n",
    "            for hit in hits:\n",
    "                docs.append(Document(page_content=hit.entity.get('content'), metadata={'content_type': hit.entity.get('content_type')}))\n",
    "        return docs\n",
    "\n",
    "# Create retrievers for each index type\n",
    "flat_retriever = MilvusRetriever(flat_collection, embedding_function)\n",
    "hnsw_retriever = MilvusRetriever(hnsw_collection, embedding_function)\n",
    "ivf_retriever = MilvusRetriever(ivf_collection, embedding_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Measure Retriever Time\n",
    "\n",
    "Compare the retrieval time for Flat, HNSW, and IVF indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_retrieval_time(retriever, query, runs=10):\n",
    "    \"\"\"Measure average retrieval time for a query.\"\"\"\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        start_time = time.time()\n",
    "        retriever.invoke(query)\n",
    "        times.append(time.time() - start_time)\n",
    "    return sum(times) / len(times)\n",
    "\n",
    "# Test query\n",
    "test_query = \"What is hypertension?\"\n",
    "\n",
    "# Measure times\n",
    "flat_time = measure_retrieval_time(flat_retriever, test_query)\n",
    "hnsw_time = measure_retrieval_time(hnsw_retriever, test_query)\n",
    "ivf_time = measure_retrieval_time(ivf_retriever, test_query)\n",
    "\n",
    "print(f\"Flat Index Retrieval Time: {flat_time:.4f} seconds\")\n",
    "print(f\"HNSW Index Retrieval Time: {hnsw_time:.4f} seconds\")\n",
    "print(f\"IVF Index Retrieval Time: {ivf_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Accuracy of Similarity Search\n",
    "\n",
    "Calculate precision@k for each index type using predefined query-result pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(retriever, test_cases, k=5):\n",
    "    \"\"\"Calculate precision@k for a retriever.\"\"\"\n",
    "    total_precision = 0\n",
    "    for query, relevant_docs in test_cases.items():\n",
    "        retrieved_docs = retriever.invoke(query, top_k=k)\n",
    "        retrieved_contents = [doc.page_content for doc in retrieved_docs]\n",
    "        relevant_count = sum(1 for doc in retrieved_contents if doc in relevant_docs)\n",
    "        precision = relevant_count / k if retrieved_docs else 0\n",
    "        total_precision += precision\n",
    "    return total_precision / len(test_cases)\n",
    "\n",
    "# Example test cases (replace with actual relevant documents from your PDFs)\n",
    "test_cases = {\n",
    "    \"What is hypertension?\": [chunked_texts[0]],  # Assume first chunk is relevant\n",
    "    \"Explain the first table\": [tables[0]],\n",
    "}\n",
    "\n",
    "# Evaluate accuracy\n",
    "flat_accuracy = evaluate_accuracy(flat_retriever, test_cases)\n",
    "hnsw_accuracy = evaluate_accuracy(hnsw_retriever, test_cases)\n",
    "ivf_accuracy = evaluate_accuracy(ivf_retriever, test_cases)\n",
    "\n",
    "print(f\"Flat Index Accuracy: {flat_accuracy:.4f}\")\n",
    "print(f\"HNSW Index Accuracy: {hnsw_accuracy:.4f}\")\n",
    "print(f\"IVF Index Accuracy: {ivf_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Reranking with BM25 and MMR\n",
    "\n",
    "Apply BM25 and MMR reranking to the retrieved results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_rerank(query, docs):\n",
    "    \"\"\"Rerank documents using BM25.\"\"\"\n",
    "    tokenized_docs = [doc.page_content.split() for doc in docs]\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    tokenized_query = query.split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    sorted_docs = [doc for _, doc in sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)]\n",
    "    return sorted_docs\n",
    "\n",
    "def mmr_rerank(query, docs, embedding_model, lambda_param=0.6):\n",
    "    \"\"\"Rerank documents using MMR.\"\"\"\n",
    "    query_embedding = embedding_model.encode(query)\n",
    "    doc_embeddings = [embedding_model.encode(doc.page_content) for doc in docs]\n",
    "    selected = []\n",
    "    remaining = list(range(len(docs)))\n",
    "    \n",
    "    # Select the document with the highest similarity to the query\n",
    "    scores = [util.cos_sim(query_embedding, doc_emb)[0][0].item() for doc_emb in doc_embeddings]\n",
    "    max_idx = scores.index(max(scores))\n",
    "    selected.append(max_idx)\n",
    "    remaining.remove(max_idx)\n",
    "    \n",
    "    # Iteratively select documents\n",
    "    while remaining:\n",
    "        mmr_scores = []\n",
    "        for i in remaining:\n",
    "            sim_to_query = util.cos_sim(query_embedding, doc_embeddings[i])[0][0].item()\n",
    "            sim_to_selected = max([util.cos_sim(doc_embeddings[i], doc_embeddings[j])[0][0].item() for j in selected], default=0)\n",
    "            mmr_score = lambda_param * sim_to_query - (1 - lambda_param) * sim_to_selected\n",
    "            mmr_scores.append(mmr_score)\n",
    "        max_idx = remaining[mmr_scores.index(max(mmr_scores))]\n",
    "        selected.append(max_idx)\n",
    "        remaining.remove(max_idx)\n",
    "    \n",
    "    return [docs[i] for i in selected]\n",
    "\n",
    "# Initialize sentence transformer for MMR\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Prompt Template\n",
    "\n",
    "Define a prompt template for the LLM to generate responses based on retrieved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"Create a prompt for the LLM with text, tables, and images.\"\"\"\n",
    "    formatted_texts = \"\\n\".join([doc.page_content for doc in data_dict['context'] if doc.metadata['content_type'] == 'text'])\n",
    "    formatted_tables = \"\\n\".join([doc.page_content for doc in data_dict['context'] if doc.metadata['content_type'] == 'table'])\n",
    "    images = [doc.page_content for doc in data_dict['context'] if doc.metadata['content_type'] == 'image']\n",
    "    \n",
    "    messages = []\n",
    "    \n",
    "    # Add images\n",
    "    for image in images:\n",
    "        messages.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"}})\n",
    "    \n",
    "    # Add text and tables\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are a helpful assistant. Use the provided information to answer the user's question accurately. The information includes text, tables, and images.\\n\"\n",
    "            f\"User question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text content:\\n\"\n",
    "            f\"{formatted_texts}\\n\\n\"\n",
    "            \"Table content:\\n\"\n",
    "            f\"{formatted_tables}\\n\\n\"\n",
    "            \"Images are provided separately. Use them if relevant to the question.\"\n",
    "        )\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    \n",
    "    return [HumanMessage(content=messages)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Multimodal RAG Chain\n",
    "\n",
    "Create a multimodal RAG chain with reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_modal_rag_chain(retriever, rerank_method='bm25'):\n",
    "    \"\"\"Create a multimodal RAG chain with reranking.\"\"\"\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o\", max_tokens=1024)\n",
    "    \n",
    "    def rerank_docs(inputs):\n",
    "        docs = inputs['context']\n",
    "        query = inputs['question']\n",
    "        if rerank_method == 'bm25':\n",
    "            return {'context': bm25_rerank(query, docs), 'question': query}\n",
    "        else:  # MMR\n",
    "            return {'context': mmr_rerank(query, docs, embedding_model), 'question': query}\n",
    "    \n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever.invoke,\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | RunnableLambda(rerank_docs)\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "# Create RAG chains for each index type with BM25 and MMR\n",
    "flat_bm25_chain = multi_modal_rag_chain(flat_retriever, 'bm25')\n",
    "flat_mmr_chain = multi_modal_rag_chain(flat_retriever, 'mmr')\n",
    "hnsw_bm25_chain = multi_modal_rag_chain(hnsw_retriever, 'bm25')\n",
    "hnsw_mmr_chain = multi_modal_rag_chain(hnsw_retriever, 'mmr')\n",
    "ivf_bm25_chain = multi_modal_rag_chain(ivf_retriever, 'bm25')\n",
    "ivf_mmr_chain = multi_modal_rag_chain(ivf_retriever, 'mmr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Generate and Render Output to DOCX\n",
    "\n",
    "Run the RAG chain and save the output to a DOCX file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_docx(query, response, docs, output_file='output.docx'):\n",
    "    \"\"\"Save query, response, and retrieved documents to a DOCX file.\"\"\"\n",
    "    doc = DocxDocument()\n",
    "    doc.add_heading('Multimodal RAG Output', 0)\n",
    "    \n",
    "    doc.add_heading('Query', level=1)\n",
    "    doc.add_paragraph(query)\n",
    "    \n",
    "    doc.add_heading('Response', level=1)\n",
    "    doc.add_paragraph(response)\n",
    "    \n",
    "    doc.add_heading('Retrieved Documents', level=1)\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        doc.add_heading(f'Document {i}', level=2)\n",
    "        if d.metadata['content_type'] == 'image':\n",
    "            img_data = base64.b64decode(d.page_content)\n",
    "            img = Image.open(io.BytesIO(img_data))\n",
    "            img.save('temp_image.jpg')\n",
    "            doc.add_picture('temp_image.jpg', width=Inches(4))\n",
    "        else:\n",
    "            doc.add_paragraph(d.page_content)\n",
    "        doc.add_paragraph(f\"Type: {d.metadata['content_type']}\")\n",
    "    \n",
    "    doc.save(output_file)\n",
    "\n",
    "# Test the RAG chain\n",
    "query = \"What is hypertension? Explain the first table in the PDF.\"\n",
    "response = ivf_bm25_chain.invoke(query)\n",
    "docs = ivf_retriever.invoke(query)\n",
    "save_to_docx(query, response, docs, 'output_hypertension.docx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
